----- C generated from akx-powers.tpl with: ------
variants = ([(4, 4, 0, 4, 4), (1, 1, 0, 1, 1)])
--------------------------------------------------

#include <Python.h>
#include <numpy/arrayobject.h>

// C headers
#include <stdlib.h> // for NULL
#include <stdio.h>  // for fprintf

#ifdef __SSE3__ // will be defined when compiling, but not when checking dependencies
#include <pmmintrin.h> // for SSE
#endif

#include "akx.h"

#ifdef _OPENMP
#include <omp.h>
#else
#include <pthread.h> // for pthreads stuff
pthread_barrier_t barrier;
#endif
















void bcsr_spmv_1_1_0_1_1(
  const struct bcsr_t *__restrict__ A,
  const value_t *__restrict__ x,
  value_t *__restrict__ y,
  index_t mb)
{
  index_t ib, jb;
  
  uint16_t *__restrict__ browptr = A->browptr16;
  uint16_t *__restrict__ bcolidx = A->bcolidx16;

  for (ib = 0; ib < mb; ++ib)
  {
    
    
      double y0 = 0.0;

    for (jb = browptr[ib]; jb < browptr[ib+1]; ++jb)
    {
      index_t j = bcolidx[jb];
      
      double x0 = x[j*1 + 0];

      
          y0 += A->bvalues[jb*1 + 0] * x0;

    }
    
      y[ib*1 + 0] = y0;


  }
}

void bcsr_spmv_rowlist_1_1_0_1_1(
  const struct bcsr_t *__restrict__ A,
  const value_t *__restrict__ x,
  value_t *__restrict__ y,
  index_t mb,
  const index_t *__restrict__ computation_seq,
  index_t seq_len)
{
  index_t q, ib, jb;
  
  uint16_t *__restrict__ browptr = A->browptr16;
  uint16_t *__restrict__ bcolidx = A->bcolidx16;

  for (q = 0; q < seq_len; q++)
  {
    ib = computation_seq[q];
    
    
      double y0 = 0.0;

    for (jb = browptr[ib]; jb < browptr[ib+1]; ++jb)
    {
      index_t j = bcolidx[jb];
      
      double x0 = x[j*1 + 0];

      
          y0 += A->bvalues[jb*1 + 0] * x0;

    }
    
      y[ib*1 + 0] = y0;


  }
}

void bcsr_spmv_stanzas_1_1_0_1_1(
  const struct bcsr_t *__restrict__ A,
  const value_t *__restrict__ x,
  value_t *__restrict__ y,
  index_t mb,
  const index_t *__restrict__ computation_seq,
  index_t seq_len)
{
  index_t q, ib, jb;
  
  uint16_t *__restrict__ browptr = A->browptr16;
  uint16_t *__restrict__ bcolidx = A->bcolidx16;

  for (q = 0; q < seq_len; q += 2)
  {
    for (ib = computation_seq[q]; ib < computation_seq[q+1]; ib++)
    {
      
    
      double y0 = 0.0;

    for (jb = browptr[ib]; jb < browptr[ib+1]; ++jb)
    {
      index_t j = bcolidx[jb];
      
      double x0 = x[j*1 + 0];

      
          y0 += A->bvalues[jb*1 + 0] * x0;

    }
    
      y[ib*1 + 0] = y0;


    }
  }
}




void bcsr_spmv_symmetric_1_1_0_1_1(
  const struct bcsr_t *__restrict__ A,
  const value_t *__restrict__ x,
  value_t *__restrict__ y,
  index_t mb)
{
  index_t ib, jb;
  
  uint16_t *__restrict__ browptr = A->browptr16;
  uint16_t *__restrict__ bcolidx = A->bcolidx16;

  for (ib = 0; ib < mb; ++ib)
  {
    
    
      double yi0 = y[ib*1 + 0];

    
      double xi0 = x[ib*1 + 0];

    for (jb = browptr[ib]; jb < browptr[ib+1]; ++jb)
    {
      index_t j = bcolidx[jb];
      
      double xj0 = x[j*1 + 0];

      
          yi0 += A->bvalues[jb*1 + 0] * xj0;

      if (j > ib && j < mb)
      {
        
      double yj0 = y[j*1 + 0];

        
          yj0 += A->bvalues[jb*1 + 0] * xi0;

        
      y[j*1 + 0] = yj0;

      }
    }
    
      y[ib*1 + 0] = yi0;


  }
}

void bcsr_spmv_symmetric_rowlist_1_1_0_1_1(
  const struct bcsr_t *__restrict__ A,
  const value_t *__restrict__ x,
  value_t *__restrict__ y,
  index_t mb,
  const index_t *__restrict__ computation_seq,
  index_t seq_len)
{
  index_t q, ib, jb;
  
  uint16_t *__restrict__ browptr = A->browptr16;
  uint16_t *__restrict__ bcolidx = A->bcolidx16;

  for (q = 0; q < seq_len; q++)
  {
    ib = computation_seq[q];
    
    
      double yi0 = y[ib*1 + 0];

    
      double xi0 = x[ib*1 + 0];

    for (jb = browptr[ib]; jb < browptr[ib+1]; ++jb)
    {
      index_t j = bcolidx[jb];
      
      double xj0 = x[j*1 + 0];

      
          yi0 += A->bvalues[jb*1 + 0] * xj0;

      if (j > ib && j < mb)
      {
        
      double yj0 = y[j*1 + 0];

        
          yj0 += A->bvalues[jb*1 + 0] * xi0;

        
      y[j*1 + 0] = yj0;

      }
    }
    
      y[ib*1 + 0] = yi0;


  }
}

void bcsr_spmv_symmetric_stanzas_1_1_0_1_1(
  const struct bcsr_t *__restrict__ A,
  const value_t *__restrict__ x,
  value_t *__restrict__ y,
  index_t mb,
  const index_t *__restrict__ computation_seq,
  index_t seq_len)
{
  index_t q, ib, jb;
  
  uint16_t *__restrict__ browptr = A->browptr16;
  uint16_t *__restrict__ bcolidx = A->bcolidx16;

  for (q = 0; q < seq_len; q += 2)
  {
    for (ib = computation_seq[q]; ib < computation_seq[q+1]; ib++)
    {
      
    
      double yi0 = y[ib*1 + 0];

    
      double xi0 = x[ib*1 + 0];

    for (jb = browptr[ib]; jb < browptr[ib+1]; ++jb)
    {
      index_t j = bcolidx[jb];
      
      double xj0 = x[j*1 + 0];

      
          yi0 += A->bvalues[jb*1 + 0] * xj0;

      if (j > ib && j < mb)
      {
        
      double yj0 = y[j*1 + 0];

        
          yj0 += A->bvalues[jb*1 + 0] * xi0;

        
      y[j*1 + 0] = yj0;

      }
    }
    
      y[ib*1 + 0] = yi0;


    }
  }
}




void bcsr_spmv_4_4_0_4_4(
  const struct bcsr_t *__restrict__ A,
  const value_t *__restrict__ x,
  value_t *__restrict__ y,
  index_t mb)
{
  index_t ib, jb;
  
  uint16_t *__restrict__ browptr = A->browptr16;
  uint16_t *__restrict__ bcolidx = A->bcolidx16;

  for (ib = 0; ib < mb; ++ib)
  {
    
    
      __m128d y0 = _mm_setzero_pd();
      __m128d y1 = _mm_setzero_pd();
      __m128d y2 = _mm_setzero_pd();
      __m128d y3 = _mm_setzero_pd();

    for (jb = browptr[ib]; jb < browptr[ib+1]; ++jb)
    {
      index_t j = bcolidx[jb];
      
      __m128d x0 = _mm_load_pd(&x[j*4 + 0]);
      __m128d x2 = _mm_load_pd(&x[j*4 + 2]);

      
          y0 = _mm_add_pd(y0, _mm_mul_pd(x0, _mm_load_pd(&A->bvalues[jb*16 + 0])));
          y0 = _mm_add_pd(y0, _mm_mul_pd(x2, _mm_load_pd(&A->bvalues[jb*16 + 2])));
          y1 = _mm_add_pd(y1, _mm_mul_pd(x0, _mm_load_pd(&A->bvalues[jb*16 + 4])));
          y1 = _mm_add_pd(y1, _mm_mul_pd(x2, _mm_load_pd(&A->bvalues[jb*16 + 6])));
          y2 = _mm_add_pd(y2, _mm_mul_pd(x0, _mm_load_pd(&A->bvalues[jb*16 + 8])));
          y2 = _mm_add_pd(y2, _mm_mul_pd(x2, _mm_load_pd(&A->bvalues[jb*16 + 10])));
          y3 = _mm_add_pd(y3, _mm_mul_pd(x0, _mm_load_pd(&A->bvalues[jb*16 + 12])));
          y3 = _mm_add_pd(y3, _mm_mul_pd(x2, _mm_load_pd(&A->bvalues[jb*16 + 14])));

    }
    
      _mm_store_sd(&y[ib*4 + 0], _mm_hadd_pd(y0, y0));
      _mm_store_sd(&y[ib*4 + 1], _mm_hadd_pd(y1, y1));
      _mm_store_sd(&y[ib*4 + 2], _mm_hadd_pd(y2, y2));
      _mm_store_sd(&y[ib*4 + 3], _mm_hadd_pd(y3, y3));


  }
}

void bcsr_spmv_rowlist_4_4_0_4_4(
  const struct bcsr_t *__restrict__ A,
  const value_t *__restrict__ x,
  value_t *__restrict__ y,
  index_t mb,
  const index_t *__restrict__ computation_seq,
  index_t seq_len)
{
  index_t q, ib, jb;
  
  uint16_t *__restrict__ browptr = A->browptr16;
  uint16_t *__restrict__ bcolidx = A->bcolidx16;

  for (q = 0; q < seq_len; q++)
  {
    ib = computation_seq[q];
    
    
      __m128d y0 = _mm_setzero_pd();
      __m128d y1 = _mm_setzero_pd();
      __m128d y2 = _mm_setzero_pd();
      __m128d y3 = _mm_setzero_pd();

    for (jb = browptr[ib]; jb < browptr[ib+1]; ++jb)
    {
      index_t j = bcolidx[jb];
      
      __m128d x0 = _mm_load_pd(&x[j*4 + 0]);
      __m128d x2 = _mm_load_pd(&x[j*4 + 2]);

      
          y0 = _mm_add_pd(y0, _mm_mul_pd(x0, _mm_load_pd(&A->bvalues[jb*16 + 0])));
          y0 = _mm_add_pd(y0, _mm_mul_pd(x2, _mm_load_pd(&A->bvalues[jb*16 + 2])));
          y1 = _mm_add_pd(y1, _mm_mul_pd(x0, _mm_load_pd(&A->bvalues[jb*16 + 4])));
          y1 = _mm_add_pd(y1, _mm_mul_pd(x2, _mm_load_pd(&A->bvalues[jb*16 + 6])));
          y2 = _mm_add_pd(y2, _mm_mul_pd(x0, _mm_load_pd(&A->bvalues[jb*16 + 8])));
          y2 = _mm_add_pd(y2, _mm_mul_pd(x2, _mm_load_pd(&A->bvalues[jb*16 + 10])));
          y3 = _mm_add_pd(y3, _mm_mul_pd(x0, _mm_load_pd(&A->bvalues[jb*16 + 12])));
          y3 = _mm_add_pd(y3, _mm_mul_pd(x2, _mm_load_pd(&A->bvalues[jb*16 + 14])));

    }
    
      _mm_store_sd(&y[ib*4 + 0], _mm_hadd_pd(y0, y0));
      _mm_store_sd(&y[ib*4 + 1], _mm_hadd_pd(y1, y1));
      _mm_store_sd(&y[ib*4 + 2], _mm_hadd_pd(y2, y2));
      _mm_store_sd(&y[ib*4 + 3], _mm_hadd_pd(y3, y3));


  }
}

void bcsr_spmv_stanzas_4_4_0_4_4(
  const struct bcsr_t *__restrict__ A,
  const value_t *__restrict__ x,
  value_t *__restrict__ y,
  index_t mb,
  const index_t *__restrict__ computation_seq,
  index_t seq_len)
{
  index_t q, ib, jb;
  
  uint16_t *__restrict__ browptr = A->browptr16;
  uint16_t *__restrict__ bcolidx = A->bcolidx16;

  for (q = 0; q < seq_len; q += 2)
  {
    for (ib = computation_seq[q]; ib < computation_seq[q+1]; ib++)
    {
      
    
      __m128d y0 = _mm_setzero_pd();
      __m128d y1 = _mm_setzero_pd();
      __m128d y2 = _mm_setzero_pd();
      __m128d y3 = _mm_setzero_pd();

    for (jb = browptr[ib]; jb < browptr[ib+1]; ++jb)
    {
      index_t j = bcolidx[jb];
      
      __m128d x0 = _mm_load_pd(&x[j*4 + 0]);
      __m128d x2 = _mm_load_pd(&x[j*4 + 2]);

      
          y0 = _mm_add_pd(y0, _mm_mul_pd(x0, _mm_load_pd(&A->bvalues[jb*16 + 0])));
          y0 = _mm_add_pd(y0, _mm_mul_pd(x2, _mm_load_pd(&A->bvalues[jb*16 + 2])));
          y1 = _mm_add_pd(y1, _mm_mul_pd(x0, _mm_load_pd(&A->bvalues[jb*16 + 4])));
          y1 = _mm_add_pd(y1, _mm_mul_pd(x2, _mm_load_pd(&A->bvalues[jb*16 + 6])));
          y2 = _mm_add_pd(y2, _mm_mul_pd(x0, _mm_load_pd(&A->bvalues[jb*16 + 8])));
          y2 = _mm_add_pd(y2, _mm_mul_pd(x2, _mm_load_pd(&A->bvalues[jb*16 + 10])));
          y3 = _mm_add_pd(y3, _mm_mul_pd(x0, _mm_load_pd(&A->bvalues[jb*16 + 12])));
          y3 = _mm_add_pd(y3, _mm_mul_pd(x2, _mm_load_pd(&A->bvalues[jb*16 + 14])));

    }
    
      _mm_store_sd(&y[ib*4 + 0], _mm_hadd_pd(y0, y0));
      _mm_store_sd(&y[ib*4 + 1], _mm_hadd_pd(y1, y1));
      _mm_store_sd(&y[ib*4 + 2], _mm_hadd_pd(y2, y2));
      _mm_store_sd(&y[ib*4 + 3], _mm_hadd_pd(y3, y3));


    }
  }
}




void bcsr_spmv_symmetric_4_4_0_4_4(
  const struct bcsr_t *__restrict__ A,
  const value_t *__restrict__ x,
  value_t *__restrict__ y,
  index_t mb)
{
  index_t ib, jb;
  
  uint16_t *__restrict__ browptr = A->browptr16;
  uint16_t *__restrict__ bcolidx = A->bcolidx16;

  for (ib = 0; ib < mb; ++ib)
  {
    
    
      __m128d yi0 = _mm_load_sd(&y[ib*4 + 0]);
      __m128d yi1 = _mm_load_sd(&y[ib*4 + 1]);
      __m128d yi2 = _mm_load_sd(&y[ib*4 + 2]);
      __m128d yi3 = _mm_load_sd(&y[ib*4 + 3]);

    
      __m128d xi0 = _mm_load1_pd(&x[ib*4 + 0]);
      __m128d xi1 = _mm_load1_pd(&x[ib*4 + 1]);
      __m128d xi2 = _mm_load1_pd(&x[ib*4 + 2]);
      __m128d xi3 = _mm_load1_pd(&x[ib*4 + 3]);

    for (jb = browptr[ib]; jb < browptr[ib+1]; ++jb)
    {
      index_t j = bcolidx[jb];
      
      __m128d xj0 = _mm_load_pd(&x[j*4 + 0]);
      __m128d xj2 = _mm_load_pd(&x[j*4 + 2]);

      
          yi0 = _mm_add_pd(yi0, _mm_mul_pd(xj0, _mm_load_pd(&A->bvalues[jb*16 + 0])));
          yi0 = _mm_add_pd(yi0, _mm_mul_pd(xj2, _mm_load_pd(&A->bvalues[jb*16 + 2])));
          yi1 = _mm_add_pd(yi1, _mm_mul_pd(xj0, _mm_load_pd(&A->bvalues[jb*16 + 4])));
          yi1 = _mm_add_pd(yi1, _mm_mul_pd(xj2, _mm_load_pd(&A->bvalues[jb*16 + 6])));
          yi2 = _mm_add_pd(yi2, _mm_mul_pd(xj0, _mm_load_pd(&A->bvalues[jb*16 + 8])));
          yi2 = _mm_add_pd(yi2, _mm_mul_pd(xj2, _mm_load_pd(&A->bvalues[jb*16 + 10])));
          yi3 = _mm_add_pd(yi3, _mm_mul_pd(xj0, _mm_load_pd(&A->bvalues[jb*16 + 12])));
          yi3 = _mm_add_pd(yi3, _mm_mul_pd(xj2, _mm_load_pd(&A->bvalues[jb*16 + 14])));

      if (j > ib && j < mb)
      {
        
      __m128d yj0 = _mm_load_pd(&y[j*4 + 0]);
      __m128d yj2 = _mm_load_pd(&y[j*4 + 2]);

        
          yj0 = _mm_add_pd(yj0, _mm_mul_pd(xi0, _mm_load_pd(&A->bvalues[jb*16 + 0])));
          yj2 = _mm_add_pd(yj2, _mm_mul_pd(xi0, _mm_load_pd(&A->bvalues[jb*16 + 2])));
          yj0 = _mm_add_pd(yj0, _mm_mul_pd(xi1, _mm_load_pd(&A->bvalues[jb*16 + 4])));
          yj2 = _mm_add_pd(yj2, _mm_mul_pd(xi1, _mm_load_pd(&A->bvalues[jb*16 + 6])));
          yj0 = _mm_add_pd(yj0, _mm_mul_pd(xi2, _mm_load_pd(&A->bvalues[jb*16 + 8])));
          yj2 = _mm_add_pd(yj2, _mm_mul_pd(xi2, _mm_load_pd(&A->bvalues[jb*16 + 10])));
          yj0 = _mm_add_pd(yj0, _mm_mul_pd(xi3, _mm_load_pd(&A->bvalues[jb*16 + 12])));
          yj2 = _mm_add_pd(yj2, _mm_mul_pd(xi3, _mm_load_pd(&A->bvalues[jb*16 + 14])));

        
      _mm_store_pd(&y[j*4 + 0], yj0);
      _mm_store_pd(&y[j*4 + 2], yj2);

      }
    }
    
      _mm_store_sd(&y[ib*4 + 0], _mm_hadd_pd(yi0, yi0));
      _mm_store_sd(&y[ib*4 + 1], _mm_hadd_pd(yi1, yi1));
      _mm_store_sd(&y[ib*4 + 2], _mm_hadd_pd(yi2, yi2));
      _mm_store_sd(&y[ib*4 + 3], _mm_hadd_pd(yi3, yi3));


  }
}

void bcsr_spmv_symmetric_rowlist_4_4_0_4_4(
  const struct bcsr_t *__restrict__ A,
  const value_t *__restrict__ x,
  value_t *__restrict__ y,
  index_t mb,
  const index_t *__restrict__ computation_seq,
  index_t seq_len)
{
  index_t q, ib, jb;
  
  uint16_t *__restrict__ browptr = A->browptr16;
  uint16_t *__restrict__ bcolidx = A->bcolidx16;

  for (q = 0; q < seq_len; q++)
  {
    ib = computation_seq[q];
    
    
      __m128d yi0 = _mm_load_sd(&y[ib*4 + 0]);
      __m128d yi1 = _mm_load_sd(&y[ib*4 + 1]);
      __m128d yi2 = _mm_load_sd(&y[ib*4 + 2]);
      __m128d yi3 = _mm_load_sd(&y[ib*4 + 3]);

    
      __m128d xi0 = _mm_load1_pd(&x[ib*4 + 0]);
      __m128d xi1 = _mm_load1_pd(&x[ib*4 + 1]);
      __m128d xi2 = _mm_load1_pd(&x[ib*4 + 2]);
      __m128d xi3 = _mm_load1_pd(&x[ib*4 + 3]);

    for (jb = browptr[ib]; jb < browptr[ib+1]; ++jb)
    {
      index_t j = bcolidx[jb];
      
      __m128d xj0 = _mm_load_pd(&x[j*4 + 0]);
      __m128d xj2 = _mm_load_pd(&x[j*4 + 2]);

      
          yi0 = _mm_add_pd(yi0, _mm_mul_pd(xj0, _mm_load_pd(&A->bvalues[jb*16 + 0])));
          yi0 = _mm_add_pd(yi0, _mm_mul_pd(xj2, _mm_load_pd(&A->bvalues[jb*16 + 2])));
          yi1 = _mm_add_pd(yi1, _mm_mul_pd(xj0, _mm_load_pd(&A->bvalues[jb*16 + 4])));
          yi1 = _mm_add_pd(yi1, _mm_mul_pd(xj2, _mm_load_pd(&A->bvalues[jb*16 + 6])));
          yi2 = _mm_add_pd(yi2, _mm_mul_pd(xj0, _mm_load_pd(&A->bvalues[jb*16 + 8])));
          yi2 = _mm_add_pd(yi2, _mm_mul_pd(xj2, _mm_load_pd(&A->bvalues[jb*16 + 10])));
          yi3 = _mm_add_pd(yi3, _mm_mul_pd(xj0, _mm_load_pd(&A->bvalues[jb*16 + 12])));
          yi3 = _mm_add_pd(yi3, _mm_mul_pd(xj2, _mm_load_pd(&A->bvalues[jb*16 + 14])));

      if (j > ib && j < mb)
      {
        
      __m128d yj0 = _mm_load_pd(&y[j*4 + 0]);
      __m128d yj2 = _mm_load_pd(&y[j*4 + 2]);

        
          yj0 = _mm_add_pd(yj0, _mm_mul_pd(xi0, _mm_load_pd(&A->bvalues[jb*16 + 0])));
          yj2 = _mm_add_pd(yj2, _mm_mul_pd(xi0, _mm_load_pd(&A->bvalues[jb*16 + 2])));
          yj0 = _mm_add_pd(yj0, _mm_mul_pd(xi1, _mm_load_pd(&A->bvalues[jb*16 + 4])));
          yj2 = _mm_add_pd(yj2, _mm_mul_pd(xi1, _mm_load_pd(&A->bvalues[jb*16 + 6])));
          yj0 = _mm_add_pd(yj0, _mm_mul_pd(xi2, _mm_load_pd(&A->bvalues[jb*16 + 8])));
          yj2 = _mm_add_pd(yj2, _mm_mul_pd(xi2, _mm_load_pd(&A->bvalues[jb*16 + 10])));
          yj0 = _mm_add_pd(yj0, _mm_mul_pd(xi3, _mm_load_pd(&A->bvalues[jb*16 + 12])));
          yj2 = _mm_add_pd(yj2, _mm_mul_pd(xi3, _mm_load_pd(&A->bvalues[jb*16 + 14])));

        
      _mm_store_pd(&y[j*4 + 0], yj0);
      _mm_store_pd(&y[j*4 + 2], yj2);

      }
    }
    
      _mm_store_sd(&y[ib*4 + 0], _mm_hadd_pd(yi0, yi0));
      _mm_store_sd(&y[ib*4 + 1], _mm_hadd_pd(yi1, yi1));
      _mm_store_sd(&y[ib*4 + 2], _mm_hadd_pd(yi2, yi2));
      _mm_store_sd(&y[ib*4 + 3], _mm_hadd_pd(yi3, yi3));


  }
}

void bcsr_spmv_symmetric_stanzas_4_4_0_4_4(
  const struct bcsr_t *__restrict__ A,
  const value_t *__restrict__ x,
  value_t *__restrict__ y,
  index_t mb,
  const index_t *__restrict__ computation_seq,
  index_t seq_len)
{
  index_t q, ib, jb;
  
  uint16_t *__restrict__ browptr = A->browptr16;
  uint16_t *__restrict__ bcolidx = A->bcolidx16;

  for (q = 0; q < seq_len; q += 2)
  {
    for (ib = computation_seq[q]; ib < computation_seq[q+1]; ib++)
    {
      
    
      __m128d yi0 = _mm_load_sd(&y[ib*4 + 0]);
      __m128d yi1 = _mm_load_sd(&y[ib*4 + 1]);
      __m128d yi2 = _mm_load_sd(&y[ib*4 + 2]);
      __m128d yi3 = _mm_load_sd(&y[ib*4 + 3]);

    
      __m128d xi0 = _mm_load1_pd(&x[ib*4 + 0]);
      __m128d xi1 = _mm_load1_pd(&x[ib*4 + 1]);
      __m128d xi2 = _mm_load1_pd(&x[ib*4 + 2]);
      __m128d xi3 = _mm_load1_pd(&x[ib*4 + 3]);

    for (jb = browptr[ib]; jb < browptr[ib+1]; ++jb)
    {
      index_t j = bcolidx[jb];
      
      __m128d xj0 = _mm_load_pd(&x[j*4 + 0]);
      __m128d xj2 = _mm_load_pd(&x[j*4 + 2]);

      
          yi0 = _mm_add_pd(yi0, _mm_mul_pd(xj0, _mm_load_pd(&A->bvalues[jb*16 + 0])));
          yi0 = _mm_add_pd(yi0, _mm_mul_pd(xj2, _mm_load_pd(&A->bvalues[jb*16 + 2])));
          yi1 = _mm_add_pd(yi1, _mm_mul_pd(xj0, _mm_load_pd(&A->bvalues[jb*16 + 4])));
          yi1 = _mm_add_pd(yi1, _mm_mul_pd(xj2, _mm_load_pd(&A->bvalues[jb*16 + 6])));
          yi2 = _mm_add_pd(yi2, _mm_mul_pd(xj0, _mm_load_pd(&A->bvalues[jb*16 + 8])));
          yi2 = _mm_add_pd(yi2, _mm_mul_pd(xj2, _mm_load_pd(&A->bvalues[jb*16 + 10])));
          yi3 = _mm_add_pd(yi3, _mm_mul_pd(xj0, _mm_load_pd(&A->bvalues[jb*16 + 12])));
          yi3 = _mm_add_pd(yi3, _mm_mul_pd(xj2, _mm_load_pd(&A->bvalues[jb*16 + 14])));

      if (j > ib && j < mb)
      {
        
      __m128d yj0 = _mm_load_pd(&y[j*4 + 0]);
      __m128d yj2 = _mm_load_pd(&y[j*4 + 2]);

        
          yj0 = _mm_add_pd(yj0, _mm_mul_pd(xi0, _mm_load_pd(&A->bvalues[jb*16 + 0])));
          yj2 = _mm_add_pd(yj2, _mm_mul_pd(xi0, _mm_load_pd(&A->bvalues[jb*16 + 2])));
          yj0 = _mm_add_pd(yj0, _mm_mul_pd(xi1, _mm_load_pd(&A->bvalues[jb*16 + 4])));
          yj2 = _mm_add_pd(yj2, _mm_mul_pd(xi1, _mm_load_pd(&A->bvalues[jb*16 + 6])));
          yj0 = _mm_add_pd(yj0, _mm_mul_pd(xi2, _mm_load_pd(&A->bvalues[jb*16 + 8])));
          yj2 = _mm_add_pd(yj2, _mm_mul_pd(xi2, _mm_load_pd(&A->bvalues[jb*16 + 10])));
          yj0 = _mm_add_pd(yj0, _mm_mul_pd(xi3, _mm_load_pd(&A->bvalues[jb*16 + 12])));
          yj2 = _mm_add_pd(yj2, _mm_mul_pd(xi3, _mm_load_pd(&A->bvalues[jb*16 + 14])));

        
      _mm_store_pd(&y[j*4 + 0], yj0);
      _mm_store_pd(&y[j*4 + 2], yj2);

      }
    }
    
      _mm_store_sd(&y[ib*4 + 0], _mm_hadd_pd(yi0, yi0));
      _mm_store_sd(&y[ib*4 + 1], _mm_hadd_pd(yi1, yi1));
      _mm_store_sd(&y[ib*4 + 2], _mm_hadd_pd(yi2, yi2));
      _mm_store_sd(&y[ib*4 + 3], _mm_hadd_pd(yi3, yi3));


    }
  }
}


typedef void (*bcsr_func_noimplicit)(
  const struct bcsr_t *__restrict__ A,
  const value_t *__restrict__ x,
  value_t *__restrict__ y,
  index_t mb);
typedef void (*bcsr_func_implicit)(
  const struct bcsr_t *__restrict__ A,
  const value_t *__restrict__ x,
  value_t *__restrict__ y,
  index_t mb,
  const index_t *__restrict__ computation_seq,
  index_t seq_len);

struct bcsr_funcs {
  index_t b_m;
  index_t b_n;
  int b_transpose;
  int browptr_comp;
  int bcolidx_comp;
  struct {
    bcsr_func_noimplicit noimplicit;
    bcsr_func_implicit implicit[2];
  } funcs[2];
} bcsr_funcs_table[] = {
  { 1, 1, 0, 1, 1,
    { { bcsr_spmv_1_1_0_1_1,
        { bcsr_spmv_rowlist_1_1_0_1_1,
          bcsr_spmv_stanzas_1_1_0_1_1 }
      },
      { bcsr_spmv_symmetric_1_1_0_1_1,
        { bcsr_spmv_symmetric_rowlist_1_1_0_1_1,
          bcsr_spmv_symmetric_stanzas_1_1_0_1_1 }
      }
    }
  },
  { 4, 4, 0, 4, 4,
    { { bcsr_spmv_4_4_0_4_4,
        { bcsr_spmv_rowlist_4_4_0_4_4,
          bcsr_spmv_stanzas_4_4_0_4_4 }
      },
      { bcsr_spmv_symmetric_4_4_0_4_4,
        { bcsr_spmv_symmetric_rowlist_4_4_0_4_4,
          bcsr_spmv_symmetric_stanzas_4_4_0_4_4 }
      }
    }
  },
};

void * do_akx ( void *__restrict__ input )
{
  struct akx_data *data = (struct akx_data*) input;

  level_t glevel = 0;
  while (glevel < data->steps)
  {
    // On the last iteration, we may do fewer than k steps.
    // To minimize redundancy, we should do the later levels, [k-#steps, k),
    // rather than the earlier levels, [0, #steps).
    level_t start = data->k - (data->steps - glevel);
    if (start < 0)
      start = 0;
    glevel -= start;

    part_id_t taskno;
    for (taskno = 0; taskno < data->ntasks; taskno++) {
      AkxBlock *__restrict__ block = data->tasks[taskno].block;
      AkxImplicitSeq *__restrict__ imp = data->tasks[taskno].imp;
      index_t V_size = data->tasks[taskno].V_size;
      value_t *V = data->tasks[taskno].V;
#define V_LOCAL(l)  (&V[(l)*V_size])
#define V_GLOBAL(l) (&data->V_global[(glevel+(l))*data->V_global_m])
      index_t i;
      // copy vector to local data using perm
      value_t *__restrict__ local = V_LOCAL(start);
      value_t *__restrict__ global = V_GLOBAL(start);
      for (i = 0; i < block->perm_size; ++i)
        local[i] = global[block->perm[i]];

      struct bcsr_funcs *bf = bcsr_funcs_table;
      while (bf->b_m != block->A_part.b_m ||
             bf->b_n != block->A_part.b_n ||
             bf->b_transpose != block->A_part.b_transpose ||
             bf->browptr_comp != block->A_part.browptr_comp ||
             bf->bcolidx_comp != block->A_part.bcolidx_comp)
      {
        bf++;
        if (bf == &bcsr_funcs_table[sizeof bcsr_funcs_table / sizeof *bcsr_funcs_table])
          abort();
      }

      level_t l;
      if (imp)
      {
        bcsr_func_implicit func = bf->funcs[block->symmetric_opt].implicit[imp->stanza];
        part_id_t ib;

        if (block->symmetric_opt)
          memset(V_LOCAL(start+1), 0, sizeof(value_t) * V_size * (block->k - start));
        for (ib = 0; ib < imp->nblocks; ib++)
        {
          for (l = start; l < block->k; l++)
          {
            index_t mb = (block->schedule[l] + block->A_part.b_m - 1) / block->A_part.b_m;
            index_t lev_start = imp->level_start[ib * block->k + l];
            index_t lev_end   = imp->level_start[ib * block->k + l + 1];
            func(
              &block->A_part,
              V_LOCAL(l),
              V_LOCAL(l+1),
              mb,
              &imp->computation_seq[lev_start],
              lev_end - lev_start);
          }
        }
        for (l = start; l < block->k; l++)
        {
          // copy vector to global data using perm
          local = V_LOCAL(l+1);
          global = V_GLOBAL(l+1);
          for (i = 0; i < block->schedule[block->k-1]; ++i)
            global[block->perm[i]] = local[i];
        }
      }
      else
      {
        bcsr_func_noimplicit func = bf->funcs[block->symmetric_opt].noimplicit;
        // Perform k SpMVs
        for (l = start; l < block->k; ++l)
        {
          if (block->symmetric_opt)
            memset(V_LOCAL(l+1), 0, sizeof(value_t) * V_size);
          func(
            &block->A_part,
            V_LOCAL(l),
            V_LOCAL(l+1),
            (block->schedule[l] + block->A_part.b_m - 1) / block->A_part.b_m);

          // copy vector to global data using perm
          local = V_LOCAL(l+1);
          global = V_GLOBAL(l+1);
          for (i = 0; i < block->schedule[block->k-1]; ++i)
            global[block->perm[i]] = local[i];
        }
      }
#undef V_GLOBAL
#undef V_LOCAL
    }

#ifdef _OPENMP
    #pragma openmp barrier
#else
    pthread_barrier_wait(&barrier);
#endif
    glevel += data->k;
  }

  return NULL;
}

static PyObject *
AkxObjectC_powers(AkxObjectC *akxobj, PyObject *args)
{
  PyArrayObject *vecs;
  if (!PyArg_ParseTuple(args, "O!", &PyArray_Type, &vecs, &PyArray_Type))
    return NULL;

  if (vecs->nd != 2
      || vecs->dimensions[1] != akxobj->matrix_size
      || vecs->strides[1] != sizeof(value_t))
  {
    PyErr_SetString(PyExc_ValueError, "vector array has wrong shape");
    return NULL;
  }


  struct akx_data *td = _ALLOC_ (akxobj->nthreads * sizeof (struct akx_data));

  part_id_t pp;
  for (pp = 0; pp < akxobj->nthreads; ++pp)
  {
    // TODO: sched. affinity stuff
    td[pp].k = akxobj->k;
    td[pp].V_global = (value_t *)vecs->data;
    td[pp].V_global_m = vecs->strides[0] / sizeof(value_t);
    td[pp].ntasks = akxobj->thread_offset[pp+1] - akxobj->thread_offset[pp];
    td[pp].tasks = &akxobj->tasks[akxobj->thread_offset[pp]];
    td[pp].steps = vecs->dimensions[0] - 1;
  }

#ifdef _OPENMP
  omp_set_num_threads(akxobj->nthreads);
  #pragma omp parallel
  {
    do_akx(&td[omp_get_thread_num()]);
  }
#else
  pthread_attr_t attr;
  P( pthread_attr_init( &attr ) );
  P( pthread_barrier_init( &barrier, NULL, akxobj->nthreads ) );
  pthread_t *threads = _ALLOC_ (akxobj->nthreads * sizeof (pthread_t));

  for (pp = 1; pp < akxobj->nthreads; ++pp)
    P( pthread_create( &threads[pp], &attr, &do_akx, (void*) &td[pp] ) );

  do_akx (&td[0]);

  for( pp = 1; pp < akxobj->nthreads; ++pp ) 
    P( pthread_join( threads[pp], NULL ) );

  _FREE_ (threads);
  P( pthread_barrier_destroy( &barrier ) );
  P( pthread_attr_destroy( &attr ) );
#endif

  _FREE_ ((void*) td );
  Py_RETURN_NONE;
}

static PyObject *
AkxObjectC_new(PyTypeObject *subtype, PyObject *args, PyObject *kwds)
{
  // Note: this does not do proper error checking (and can't; we don't have
  // access to &AkxBlock_Type here as we would need to prevent arbitrary
  // objects from getting passed off as blocks).
  // We rely on the Python code to ensure parameters are sane.

  int k, matrix_size;
  PyObject *list;
  if (!PyArg_ParseTuple(args, "iiO", &k, &matrix_size, &list))
    return NULL;

  AkxObjectC *self = PyObject_New(AkxObjectC, subtype);
  if (!self)
    return NULL;

  self->k = k;
  self->matrix_size = matrix_size;

  self->nthreads = PyList_GET_SIZE(list);
  self->thread_offset = _ALLOC_((self->nthreads + 1) * sizeof(part_id_t));
  part_id_t total_blocks = 0;
  part_id_t thread;
  for (thread = 0; thread < self->nthreads; thread++)
  {
    self->thread_offset[thread] = total_blocks;
    total_blocks += PyList_GET_SIZE(PyList_GET_ITEM(list, thread));
  }
  self->thread_offset[thread] = total_blocks;

  self->tasks = _ALLOC_(total_blocks * sizeof(struct akx_task));
  for (thread = 0; thread < self->nthreads; thread++)
  {
    PyObject *sublist = PyList_GET_ITEM(list, thread);
    part_id_t j;
    for (j = 0; j < PyList_GET_SIZE(sublist); j++)
    {
      struct akx_task *task = &self->tasks[self->thread_offset[thread] + j];

      PyObject *item = PyList_GET_ITEM(sublist, j);
      AkxBlock *block;
      AkxImplicitSeq *imp = NULL;
      if (PyTuple_CheckExact(item))
      {
        block = (AkxBlock *)PyTuple_GET_ITEM(item, 0);
        Py_INCREF(block);
        imp = (AkxImplicitSeq *)PyTuple_GET_ITEM(item, 1);
        Py_INCREF(imp);
      }
      else
      {
        block = (AkxBlock *)item;
        Py_INCREF(block);
      }

      assert(block->k == k);

      task->block = block;
      task->imp = imp;

      // Expand V to accommodate padding
      index_t padded_height = block->A_part.mb * block->A_part.b_m;
      index_t padded_width = block->A_part.nb * block->A_part.b_n;
      task->V_size = (padded_height > padded_width ? padded_height : padded_width);
      task->V = _ALLOC_ ((block->k+1) * task->V_size * sizeof (value_t));
      // Don't let Inf/NaN sneak into the padding by chance
      memset(task->V, 0, (block->k+1) * task->V_size * sizeof (value_t));
    }
  }
  
  return (PyObject *)self;
}

static void
AkxObjectC_dealloc(AkxObjectC *akxobj)
{
  index_t i;
  for (i = 0; i < akxobj->thread_offset[akxobj->nthreads]; i++)
  {
    Py_DECREF(akxobj->tasks[i].block);
    Py_XDECREF(akxobj->tasks[i].imp);
    _FREE_(akxobj->tasks[i].V);
  }
  _FREE_(akxobj->tasks);
  _FREE_(akxobj->thread_offset);
  PyObject_Del(akxobj);
}

#define METHOD(name, flags) { #name, (PyCFunction)AkxObjectC_##name, flags },
static PyMethodDef AkxObjectC_methods[] = {
  METHOD(powers, METH_VARARGS)
  { NULL, NULL, 0, NULL }
};
#undef METHOD

static PyTypeObject AkxObjectC_Type = {
	PyObject_HEAD_INIT(NULL)
	0,                          /*tp_size*/
	"AkxObjectC",               /*tp_name*/
	sizeof(AkxObjectC),         /*tp_basicsize*/
	0,                          /*tp_itemsize*/
	/* methods */
	(destructor)AkxObjectC_dealloc,     /*tp_dealloc*/
	0,                          /*tp_print*/
	0,                          /*tp_getattr*/
	0,                          /*tp_setattr*/
	0,                          /*tp_compare*/
	0,                          /*tp_repr*/
	0,                          /*tp_as_number*/
	0,                          /*tp_as_sequence*/
	0,                          /*tp_as_mapping*/
	0,                          /*tp_hash*/
	0,                          /*tp_call*/
	0,                          /*tp_str*/
	0,                          /*tp_getattro*/
	0,                          /*tp_setattro*/
	0,                          /*tp_as_buffer*/
	Py_TPFLAGS_DEFAULT,         /*tp_flags*/
	0,                          /*tp_doc*/
	0,                          /*tp_traverse*/
	0,                          /*tp_clear*/
	0,                          /*tp_richcompare*/
	0,                          /*tp_weaklistoffset*/
	0,                          /*tp_iter*/
	0,                          /*tp_iternext*/
	AkxObjectC_methods,          /*tp_methods*/
	0,                          /*tp_members*/
	0,                          /*tp_getset*/
	0,                          /*tp_base*/
	0,                          /*tp_dict*/
	0,                          /*tp_descr_get*/
	0,                          /*tp_descr_set*/
	0,                          /*tp_dictoffset*/
	0,                          /*tp_init*/
	0,                          /*tp_alloc*/
	AkxObjectC_new,              /*tp_new*/
};

static PyMethodDef methods[] = {
	{ NULL, NULL, 0, NULL }
};

PyMODINIT_FUNC
init_akx_powers(void)
{
  PyObject *module = Py_InitModule("_akx_powers", methods);
  if (!module)
    return;

  if (PyType_Ready(&AkxObjectC_Type) < 0)
    return;

  Py_INCREF(&AkxObjectC_Type);
  PyModule_AddObject(module, "AkxObjectC", (PyObject *)&AkxObjectC_Type);

  import_array();
}
